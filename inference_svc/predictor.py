from flask import Flask, request, jsonify
import logging
import onnxruntime as ort
import numpy as np
import os

# Set up logging
logging.basicConfig(level=logging.INFO)

app = Flask(__name__)

# --- Configuration ---
# Path to the ONNX model generated by train_rf_model.py
MODEL_PATH = os.getenv("MODEL_PATH", "./models/fault-prediction/1/model.onnx")

class Predictor:
    def __init__(self, model_path):
        self.session = None
        if os.path.exists(model_path):
            try:
                self.session = ort.InferenceSession(model_path)
                logging.info(f"Loaded ONNX model from {model_path}")
            except Exception as e:
                logging.error(f"Failed to load ONNX model: {e}")
        else:
            logging.warning(f"Model file not found at {model_path}. Predictor will return mock data.")

    def predict(self, instances):
        if not self.session:
            # Fallback mock logic if no model is found
            return ["nf"] * len(instances)
        
        try:
            input_name = self.session.get_inputs()[0].name
            # Convert instances to float32 numpy array
            input_data = np.array(instances, dtype=np.float32)
            label_name = self.session.get_outputs()[0].name
            #logging.warning(f"Sent input data {input_name} : {input_data}")
            # Run inference
            # skl2onnx for RandomForestClassifier returns [labels, probabilities]
            outputs = self.session.run([label_name], {input_name: input_data})
            
            # The first output is the array of predicted labels (e.g., ["nf", "gf", ...])
            return outputs[0].tolist()
        except Exception as e:
            logging.error(f"Inference error: {e}")
            return None

# Initialize the predictor
predictor = Predictor(MODEL_PATH)

@app.route('/predict', methods=['POST'])
def get_prediction():
    """
    KServe-compliant prediction endpoint.
    Expects JSON: {"instances": [[feature1, feature2, ...]]}
    Returns: {"predictions": ["nf"]}
    """
    try:
        data = request.json
        if not data or "instances" not in data:
            return jsonify({"error": "Invalid payload. Expected 'instances' key."}), 400

        instances = data["instances"]
        app.logger.info(f"Received request for {len(instances)} instances.")
        #app.logger.info(f"Instance content {instances}")

        predictions = predictor.predict(instances)
        
        if predictions is None:
            return jsonify({"error": "Model inference failed"}), 500

        return jsonify({"predictions": predictions})

    except Exception as e:
        app.logger.error(f"Error in /predict endpoint: {e}")
        return jsonify({"error": "Internal server error"}), 500

if __name__ == '__main__':
    # host='0.0.0.0' makes it accessible from other containers/machines
    app.run(debug=False, host='0.0.0.0', port=5000)